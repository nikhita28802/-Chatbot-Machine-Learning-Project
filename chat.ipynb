{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\APPLE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = 'data/Bitext_Sample_Customer_Service_Training_Dataset.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>utterance</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM</td>\n",
       "      <td>I have problems with canceling an order</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIM</td>\n",
       "      <td>how can I find information about canceling ord...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>I need help with canceling the last order</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIP</td>\n",
       "      <td>could you help me cancelling the last order I ...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>problem with cancelling an order I made</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flags                                          utterance category  \\\n",
       "0    BM            I have problems with canceling an order    ORDER   \n",
       "1   BIM  how can I find information about canceling ord...    ORDER   \n",
       "2     B          I need help with canceling the last order    ORDER   \n",
       "3   BIP  could you help me cancelling the last order I ...    ORDER   \n",
       "4     B            problem with cancelling an order I made    ORDER   \n",
       "\n",
       "         intent  \n",
       "0  cancel_order  \n",
       "1  cancel_order  \n",
       "2  cancel_order  \n",
       "3  cancel_order  \n",
       "4  cancel_order  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters and punctuation\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['utterance'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>utterance</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM</td>\n",
       "      <td>I have problems with canceling an order</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>problems canceling order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIM</td>\n",
       "      <td>how can I find information about canceling ord...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>find information canceling orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>I need help with canceling the last order</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>need help canceling last order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIP</td>\n",
       "      <td>could you help me cancelling the last order I ...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>could help cancelling last order made</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flags                                          utterance category  \\\n",
       "0    BM            I have problems with canceling an order    ORDER   \n",
       "1   BIM  how can I find information about canceling ord...    ORDER   \n",
       "2     B          I need help with canceling the last order    ORDER   \n",
       "3   BIP  could you help me cancelling the last order I ...    ORDER   \n",
       "\n",
       "         intent                           cleaned_text  \n",
       "0  cancel_order               problems canceling order  \n",
       "1  cancel_order      find information canceling orders  \n",
       "2  cancel_order         need help canceling last order  \n",
       "3  cancel_order  could help cancelling last order made  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: I have problems with canceling an order -> Intent: cancel_order\n",
      "User Query: how can I find information about canceling orders? -> Intent: cancel_order\n",
      "User Query: I need help with canceling the last order -> Intent: cancel_order\n",
      "User Query: could you help me cancelling the last order I made? -> Intent: cancel_order\n",
      "User Query: problem with cancelling an order I made -> Intent: cancel_order\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# extract featuyres and labels\n",
    "\n",
    "utterances = df['utterance'].values\n",
    "intents = df['intent'].values\n",
    "categories=df['category'].values\n",
    "\n",
    "## Create a dictionary to display the queries and intents for inspection\n",
    "for utterance, intent in zip(utterances[:5], intents[:5]):\n",
    "    print(f\"User Query: {utterance} -> Intent: {intent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 6540\n",
      "Testing set size: 1635\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['intent'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (6540, 578)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "\n",
    "# Fit and transform the training data into TF-IDF vectors\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [11  7  1 14 19]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels for training\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"Encoded labels: {y_train_encoded[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: could I check if there is anything wrong with my refund? \n",
      "Predicted Response/Intent: track_refund\n"
     ]
    }
   ],
   "source": [
    "#Retrieval - based model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the training data\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "# Define a function to retrieve the most similar response\n",
    "def get_response(query):\n",
    "    # Transform the user query to TF-IDF\n",
    "    query_tfidf = vectorizer.transform([query]).toarray()\n",
    "    \n",
    "    # Compute cosine similarity between the query and all training utterances\n",
    "    similarities = cosine_similarity(query_tfidf, X_train_tfidf)\n",
    "    \n",
    "    # Get the index of the most similar utterance\n",
    "    max_sim_index = np.argmax(similarities)\n",
    "    \n",
    "    # Return the corresponding response (intent or action)\n",
    "    return y_train.iloc[max_sim_index]\n",
    "\n",
    "# Test the retrieval-based model\n",
    "query = \"could I check if there is anything wrong with my refund?\"\n",
    "response = get_response(query)\n",
    "print(f\"User Query: {query} \\nPredicted Response/Intent: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\APPLE\\Desktop\\nikhita\\Machine Learning Project\\ML\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: I do not know howI can change to a different account? \n",
      "Generated Response: I do not know howI can change to a different account?\n",
      "\n",
      "I am not sure how to change to a different account.\n",
      "\n",
      "I am not sure how to change to a different account.\n",
      "\n",
      "I am not sure how to change\n"
     ]
    }
   ],
   "source": [
    "# Transformerws -based models\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Replace 'your_token_here' with your actual Hugging Face token\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize input and generate a response\n",
    "input_text = \"I do not know howI can change to a different account?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate response using GPT-2\n",
    "response_ids = model.generate(input_ids, max_length=50)\n",
    "response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"User Query: {input_text} \\nGenerated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 8.221833772233233e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\APPLE\\Desktop\\nikhita\\Machine Learning Project\\ML\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Quantitative Evaluation mertics\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Reference response (correct response)\n",
    "reference = [['I', 'can', 'help', 'you', 'cancel', 'your', 'order']]\n",
    "\n",
    "# Generated response from the chatbot\n",
    "candidate = ['I', 'can', 'help', 'cancel', 'your', 'order']\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = sentence_bleu(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/gpt2_model\\\\tokenizer_config.json',\n",
       " 'model/gpt2_model\\\\special_tokens_map.json',\n",
       " 'model/gpt2_model\\\\vocab.json',\n",
       " 'model/gpt2_model\\\\merges.txt',\n",
       " 'model/gpt2_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('model/gpt2_model')\n",
    "tokenizer.save_pretrained('model/gpt2_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
